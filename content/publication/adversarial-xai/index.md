+++
title = "Towards Understanding and Comparing Adversarial Robustness for Local Interpretable Methods"
date = 2021-07-31T00:00:00

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["Gaurav Parashar", "admin", "Chiranjib Bhattacharyya"]

# Publication type.
# Legend:
# 0 = Uncategorized
# 1 = Conference paper
# 2 = Journal article
# 3 = Manuscript
# 4 = Report
# 5 = Book
# 6 = Book section
publication_types = ["0"]

# Publication name and optional abbreviated version.
publication = "In Progress"
publication_short = ""

# Abstract.
# abstract = ""

# Summary. An optional shortened abstract.
summary = ""

# Digital Object Identifier (DOI)
# doi = "10.1038/s41598-020-61289-4"

# Is this a featured publication? (true/false)
featured = false

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = ["Deep Learning", "Computer Vision", "AI for healthcare", "Segmentation", "Domain Adaptation", "Niramai"]

# Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["deep-learning"]` references
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
# projects = []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references
#   `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides = "example-slides"

# Links (optional).
url_pdf = ""
url_preprint = "Adversarial_Explainability.pdf"
url_code = ""
url_dataset = ""
url_project = ""
url_slides = ""
url_video = ""
url_poster = ""
url_source = ""

# Custom links (optional).
# For multiple links, use the form `[{...}, {...}, {...}]`.
# links = [{name = "Supplementary Material", url = "https://static-content.springer.com/esm/art%3A10.1038%2Fs41598-020-61289-4/MediaObjects/41598_2020_61289_MOESM1_ESM.pdf"},
#{name = "Poster (earlier version)", url = "PosterA0.pdf"}]

# Does this page contain LaTeX math? (true/false)
math = false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
[image]
  # Caption (optional)
  caption = "(a) and (b) show that Vanishing Heatmap Attack (VHA), our proposed targetted adversarial attack for interpretable methods, can successfully attack strong adversarial defences like an ensemble of interpretable methods. Our proposed Random Perturbation-based Attack (RPA) can also attack interpretable heatmaps but without any targetted training. RPA simply modifies brightness (as shown in (c)) or contrast (as shown in (d)) to bring significant change in heatmap with little-to-no change in class predictions."

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = "BottomLeft"

+++

With the increasing use of the complex Deep Learning models, for various applications, the urge to get their explanations is also increasing. As a result there are huge number of Explainability Methods out there, and with little to no comparison made on them, leaves the practitioner in a dilemma about which one to choose. Recent works show that simple adversarial attacks on these XAI methods are possible, hence increasing the need to get an Explainability Method, which can be safely relied upon. In this work, we try to compare different Explainability Methods, and come up with a metric which compares the robustness of various XAI methods, under a given Adversarial attack framework. Observing the explanations generated by the most robust XAI method, we propose a new adversarial attack framework, The Vanishing Heatmap Attack, and our experiments confirm that even the robust XAI methods are also at vulnerability. This VHA framework can be used to perform sanity tests on novel XAI methods. Our experiments also confirm that explanations for Deep Learning solutions which consume noise-sensitive data, can easily be manipulated by random small perturbations, and there is an urgent need for a really robust and accurate Explainability Method.